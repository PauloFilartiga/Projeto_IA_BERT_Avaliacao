{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "dcTAvJvGf-l7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas scikit-learn torch"
      ],
      "metadata": {
        "collapsed": true,
        "id": "PA3xeK4bgTdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "b4p7H0A2r3cq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Preparação dos Dados\n",
        "\n",
        "CLASSES_SENTIMENTO = ['positivo', 'neutro', 'negativo', 'boa']\n",
        "CLASSES_FILTRO = ['onca', 'caseiro', 'fake news']\n",
        "\n",
        "MODEL_NAME = 'neuralmind/bert-base-portuguese-cased'\n",
        "NUM_CLASSES = 3\n",
        "\n",
        "# mapeando\n",
        "label_map = {'negativo': 0, 'neutro': 1, 'positivo': 2}\n",
        "id_to_label = {0: 'negativo', 1: 'neutro', 2: 'positivo'}\n",
        "CLASSES_FOCADAS = list(label_map.keys())\n",
        "\n",
        "try:\n",
        "  df = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/data.csv', sep=',')\n",
        "except FileNotFoundError:\n",
        "  print(\"ERRO: o arquivo 'data.csv' não foi encontrado.\")\n",
        "  raise\n",
        "\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "def get_sentiment_label(row):\n",
        "  for col in CLASSES_FILTRO:\n",
        "    sentiment_string = str(row[col]).lower().strip()\n",
        "    if sentiment_string in CLASSES_SENTIMENTO:\n",
        "      return sentiment_string\n",
        "\n",
        "  outras_cols = ['notícia', 'ironia', 'Conservacionista']\n",
        "  for col in outras_cols:\n",
        "    sentiment_string = str(row.get(col, '')).lower().strip()\n",
        "    if sentiment_string in CLASSES_SENTIMENTO:\n",
        "      return sentiment_string\n",
        "\n",
        "  return pd.NA\n",
        "\n",
        "df['sentiment'] = df.apply(get_sentiment_label, axis=1)\n",
        "\n",
        "# Ação Corretiva: Mudar 'boa' para 'positivo' nos dados ANTES do mapeamento\n",
        "df['sentiment'] = df['sentiment'].replace({'boa': 'positivo'})\n",
        "\n",
        "# Limpeza\n",
        "df.dropna(subset=['sentiment', 'comment_text'], inplace=True)\n",
        "df = df.copy()\n",
        "\n",
        "df.drop_duplicates(subset=['comment_text'], inplace=True)\n",
        "df = df[df['comment_text'].str.strip() != '']\n",
        "\n",
        "# conversão para ID\n",
        "df['label_id'] = df['sentiment'].map(label_map)\n",
        "df = df[df['label_id'].notna()]\n",
        "df.dropna(subset=['label_id'], inplace=True)\n",
        "df['label_id'] = df['label_id'].astype(int)\n",
        "\n",
        "# divisão\n",
        "if len(df) < 6:\n",
        "  print(f\"ERRO: A filtragem resultou em apenas {len(df)} amostras.\")\n",
        "  raise ValueError(\"Dados insuficientes para classificação\")\n",
        "\n",
        "# (70% treino, 15% validação, 15% teste)\n",
        "try:\n",
        "  df_train_val, df_test = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label_id'])\n",
        "  df_train, df_val = train_test_split(df_train_val, test_size=(0.15 / 0.85), random_state=42, stratify=df_train_val['label_id'])\n",
        "except ValueError as e:\n",
        "\n",
        "    print(\"\\nAVISO: Não foi possível aplicar 'stratify'.\")\n",
        "    df_train_val, df_test = train_test_split(df, test_size=0.15, random_state=42)\n",
        "    df_train, df_val = train_test_split(df_train_val, test_size=(0.15 / 0.85), random_state=42)\n",
        "\n",
        "print(f\"Número de classes de Sentimentos: {NUM_CLASSES} ({CLASSES_FOCADAS})\")\n",
        "print(f\"Total de amostras: {len(df)}\")\n",
        "print(f\"Treino: {len(df_train)} amostras | Validação: {len(df_val)} amostras | Teste: {len(df_test)} amostras\")\n"
      ],
      "metadata": {
        "id": "5zJvlopP_72e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Tokenização\n",
        "\n",
        "MAX_LEN = 128\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "class NewsDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, encodings, labels):\n",
        "    self.encodings = encodings\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "    item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "def tokenize_data(data):\n",
        "  return tokenizer.batch_encode_plus(\n",
        "      data['comment_text'].tolist(),\n",
        "      max_length=MAX_LEN,\n",
        "      padding='max_length',\n",
        "      truncation=True,\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "# Tokenizar e criar Datasets\n",
        "train_encodings = tokenize_data(df_train)\n",
        "val_encodings = tokenize_data(df_val)\n",
        "test_encodings = tokenize_data(df_test)\n",
        "\n",
        "\n",
        "train_dataset = NewsDataset(train_encodings, df_train['label_id'].tolist())\n",
        "val_dataset = NewsDataset(val_encodings, df_val['label_id'].tolist())\n",
        "test_dataset = NewsDataset(test_encodings, df_test['label_id'].tolist())\n",
        "\n",
        "print(\"Tokenização concluída e Datasets criados com sucesso.\")\n",
        "print(f\"Estrutura do Dataset de Treino: {len(train_dataset)} amostras\")\n",
        "print(f\"Estrutura do Dataset de Validação: {len(val_dataset)} amostras\")"
      ],
      "metadata": {
        "id": "f3qi_G2G1MM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Modelo / 4.Treinamento\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# carregar o modelo BERT\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=NUM_CLASSES\n",
        "    ).to(device)\n",
        "\n",
        "def compute_metrics(p):\n",
        "  preds = np.argmax(p.predictions, axis=1)\n",
        "  return {'accuracy': accuracy_score(p.label_ids, preds)}\n",
        "\n",
        "# configuração de teinamento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='.logs',\n",
        "    logging_steps=50,\n",
        "    learning_rate=2e-5,\n",
        "    load_best_model_at_end=False,\n",
        "    report_to=\"none\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "# iniciar o Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"\\nIniciando o treinamento (pode levar vários minutos)...\")\n",
        "\n",
        "all_eval_metrics = []\n",
        "all_train_loss = []\n",
        "total_epochs = 10\n",
        "\n",
        "for epoch in range(1, total_epochs + 1):\n",
        "  print(f\"\\n--- Época {epoch}/{total_epochs} ---\")\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  # coleta o Loss de treino\n",
        "  train_logs = pd.DataFrame(trainer.state.log_history)\n",
        "  # filtra e pega o último log de 'loss' de treino\n",
        "  last_train_log = train_logs[train_logs['loss'].notna()].iloc[-1]\n",
        "  last_train_loss = last_train_log['loss']\n",
        "  all_train_loss.append(last_train_loss)\n",
        "\n",
        "  eval_metrics = trainer.evaluate()\n",
        "  all_eval_metrics.append(eval_metrics)\n",
        "\n",
        "  eval_loss = eval_metrics['eval_loss']\n",
        "  eval_accuracy = eval_metrics['eval_accuracy']\n",
        "\n",
        "  print(f\"Loss Treino (Último Step): {last_train_loss:.4f}\")\n",
        "  print(f\"Loss Validação: {eval_loss:.4f} | Acurácia Validação: {eval_accuracy:.4f}\")\n",
        "\n",
        "# mostrar logs de resultadso\n",
        "print(\"\\n--- Resultados do Treinamento ---\")\n",
        "results_df = pd.DataFrame({\n",
        "    'Época': range(1, total_epochs + 1),\n",
        "    'Loss Treino': all_train_loss,\n",
        "    'Loss Validação': [m['eval_loss'] for m in all_eval_metrics],\n",
        "    'Acurácia Validação': [m['eval_accuracy'] for m in all_eval_metrics]\n",
        "})\n",
        "print(results_df)\n",
        "\n",
        "# plotar o gráfico de evolução do Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_df['Época'], results_df['Loss Treino'], label='Treino Loss', linestyle='--')\n",
        "plt.plot(results_df['Época'], results_df['Loss Validação'], label='Validação Loss', marker='o')\n",
        "plt.title('Evolução do Loss (Treino vs. Validação)')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1w14VKno9dpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Avaliação\n",
        "\n",
        "# avaliação no conjunto de teste\n",
        "print(\"\\n--- Iniciando Predições no Conjunto de Teste ---\")\n",
        "\n",
        "predictions = trainer.predict(test_dataset)\n",
        "test_preds = np.argmax(predictions.predictions, axis=1)\n",
        "test_true = predictions.label_ids\n",
        "\n",
        "class_ids = list(id_to_label.keys())\n",
        "\n",
        "# mostrar o Classification Report\n",
        "print(\"\\n--- Relatório de Classificação no Conjunto de Teste ---\")\n",
        "print(classification_report(\n",
        "    test_true,\n",
        "    test_preds,\n",
        "    target_names=list(id_to_label.values()),\n",
        "    digits=4,\n",
        "    zero_division=0\n",
        "))\n",
        "\n",
        "print(f\"Acurácia Geral no Teste: {accuracy_score(test_true, test_preds):.4f}\\n\")\n",
        "\n",
        "# mostrar exemplos de Erros\n",
        "df_test_final = df_test.copy()\n",
        "df_test_final['predicted_id'] = test_preds\n",
        "df_test_final['predicted_label'] = df_test_final['predicted_id'].map(id_to_label)\n",
        "\n",
        "errors = df_test_final[df_test_copy['label_id'] != df_test_final['predicted_id']]\n",
        "print(f\"\\n--- {len(errors)} Exemplos de Erros de Classificação ---\")\n",
        "\n",
        "\n",
        "print(\"Cinco Erros Aleatórios:\")\n",
        "if len(errors) > 0:\n",
        "  for index, row in errors.sample(min(5, len(errors))).iterrows():\n",
        "    print(f\"Texto: **{row['comment_text'][:80]}...**\")\n",
        "    print(f\" Rótulo Real: {row['sentiment']}\")\n",
        "    print(f\" Predição: {row['predicted_label']}\\n\")\n",
        "else:\n",
        "  print(\"Nenhum erro encontrado.\")\n"
      ],
      "metadata": {
        "id": "bR5HJvf_E-y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
